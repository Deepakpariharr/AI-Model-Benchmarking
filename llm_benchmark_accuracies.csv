Name,Release date,Training compute (FLOP),Training compute notes,Accuracy,Benchmark,Evaluation setting,Temperature,Top-p,Run number,Organization,Country,Accessibility,API,API name
Claude 2,2023-07-11,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.303030303,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3484848485,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2,2023-07-11,,,0.404040404,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-2.0
Claude 2.1,2023-11-21,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.398989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3888888889,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3535353535,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 2.1,2023-11-21,,,0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-2.1
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5353535354,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5606060606,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5303030303,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5303030303,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5404040404,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5454545455,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5050505051,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5505050505,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5454545455,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5404040404,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5505050505,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5404040404,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5555555556,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5151515152,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5353535354,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5353535354,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5656565657,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.4949494949,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5353535354,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5606060606,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5454545455,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5505050505,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5858585859,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5202020202,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.595959596,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5656565657,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.595959596,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5909090909,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5656565657,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5454545455,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5454545455,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5252525253,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5505050505,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5909090909,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5555555556,GPQA (Diamond Set),Simple-eval,1.0,1.0,21,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.6262626263,GPQA (Diamond Set),Simple-eval,1.0,1.0,22,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5505050505,GPQA (Diamond Set),Simple-eval,1.0,1.0,23,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Simple-eval,1.0,1.0,24,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,25,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Simple-eval,1.0,1.0,26,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5656565657,GPQA (Diamond Set),Simple-eval,1.0,1.0,27,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5404040404,GPQA (Diamond Set),Simple-eval,1.0,1.0,28,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5454545455,GPQA (Diamond Set),Simple-eval,1.0,1.0,29,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5505050505,GPQA (Diamond Set),Simple-eval,1.0,1.0,30,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5101010101,GPQA (Diamond Set),Simple-eval,1.0,1.0,31,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,32,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,33,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5858585859,GPQA (Diamond Set),Simple-eval,1.0,1.0,34,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5303030303,GPQA (Diamond Set),Simple-eval,1.0,1.0,35,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Simple-eval,1.0,1.0,36,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5707070707,GPQA (Diamond Set),Simple-eval,1.0,1.0,37,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5202020202,GPQA (Diamond Set),Simple-eval,1.0,1.0,38,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.5303030303,GPQA (Diamond Set),Simple-eval,1.0,1.0,39,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.6060606061,GPQA (Diamond Set),Simple-eval,1.0,1.0,40,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
Claude 3 Haiku,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3838383838,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.297979798,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3131313131,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3484848485,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3181818182,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3383838384,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.303030303,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3888888889,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3636363636,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3333333333,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3333333333,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3181818182,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3131313131,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.297979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3434343434,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.297979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3888888889,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Haiku,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Claude 3 Opus,2024-03-04,,,0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.5555555556,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.5101010101,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.5050505051,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.4595959596,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.4444444444,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.5101010101,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.4696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Opus,2024-03-04,,,0.5101010101,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
Claude 3 Sonnet,2024-03-04,,,0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3434343434,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.398989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3888888889,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3383838384,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3939393939,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3888888889,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.4343434343,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3636363636,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3636363636,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.4242424242,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3686868687,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.4242424242,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3939393939,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3383838384,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.4696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Claude 3 Sonnet,2024-03-04,,,0.3939393939,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3131313131,GPQA (Diamond Set),Llama-eval,0.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3383838384,GPQA (Diamond Set),Llama-eval,0.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.297979798,GPQA (Diamond Set),Llama-eval,0.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3333333333,GPQA (Diamond Set),Simple-eval,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3484848485,GPQA (Diamond Set),Simple-eval,1.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3333333333,GPQA (Diamond Set),Simple-eval,1.0,0.95,6,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3383838384,GPQA (Diamond Set),Simple-eval,1.0,0.95,7,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3383838384,GPQA (Diamond Set),Simple-eval,1.0,0.95,8,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,0.95,9,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,0.95,10,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.297979798,GPQA (Diamond Set),Simple-eval,1.0,0.95,11,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3434343434,GPQA (Diamond Set),Simple-eval,1.0,0.95,12,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3636363636,GPQA (Diamond Set),Simple-eval,1.0,0.95,13,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3434343434,GPQA (Diamond Set),Simple-eval,1.0,0.95,14,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3434343434,GPQA (Diamond Set),Simple-eval,1.0,0.95,15,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,0.95,16,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,0.95,17,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3383838384,GPQA (Diamond Set),Simple-eval,1.0,0.95,18,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3282828283,GPQA (Diamond Set),Simple-eval,1.0,0.95,19,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Gemini 1.0 Pro (2024-04-09),2024-04-09,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",0.3131313131,GPQA (Diamond Set),Simple-eval,1.0,0.95,20,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.0-pro-002
Mistral Large 2,2024-07-24,,,0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4545454545,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4696969697,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,6,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4848484848,GPQA (Diamond Set),Simple-eval,0.7,1.0,7,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5050505051,GPQA (Diamond Set),Simple-eval,0.7,1.0,8,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5050505051,GPQA (Diamond Set),Simple-eval,0.7,1.0,9,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5252525253,GPQA (Diamond Set),Simple-eval,0.7,1.0,10,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4646464646,GPQA (Diamond Set),Simple-eval,0.7,1.0,11,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5,GPQA (Diamond Set),Simple-eval,0.7,1.0,12,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4393939394,GPQA (Diamond Set),Simple-eval,0.7,1.0,13,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5202020202,GPQA (Diamond Set),Simple-eval,0.7,1.0,14,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,15,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4747474747,GPQA (Diamond Set),Simple-eval,0.7,1.0,16,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5050505051,GPQA (Diamond Set),Simple-eval,0.7,1.0,17,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5303030303,GPQA (Diamond Set),Simple-eval,0.7,1.0,18,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4343434343,GPQA (Diamond Set),Simple-eval,0.7,1.0,19,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5555555556,GPQA (Diamond Set),Simple-eval,0.7,1.0,20,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4696969697,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4545454545,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4545454545,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4444444444,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4646464646,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4696969697,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5050505051,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4444444444,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4646464646,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5101010101,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4696969697,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Simple-eval,0.7,1.0,21,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,22,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4343434343,GPQA (Diamond Set),Simple-eval,0.7,1.0,23,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Simple-eval,0.7,1.0,24,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4646464646,GPQA (Diamond Set),Simple-eval,0.7,1.0,25,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4696969697,GPQA (Diamond Set),Simple-eval,0.7,1.0,26,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Simple-eval,0.7,1.0,27,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,28,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,29,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4797979798,GPQA (Diamond Set),Simple-eval,0.7,1.0,30,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5,GPQA (Diamond Set),Simple-eval,0.7,1.0,31,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,32,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4747474747,GPQA (Diamond Set),Simple-eval,0.7,1.0,33,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4545454545,GPQA (Diamond Set),Simple-eval,0.7,1.0,34,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,35,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4797979798,GPQA (Diamond Set),Simple-eval,0.7,1.0,36,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4898989899,GPQA (Diamond Set),Simple-eval,0.7,1.0,37,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,38,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.5,GPQA (Diamond Set),Simple-eval,0.7,1.0,39,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral Large 2,2024-07-24,,,0.4848484848,GPQA (Diamond Set),Simple-eval,0.7,1.0,40,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Mistral 7B,2023-10-10,,,0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2575757576,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.303030303,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2878787879,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2272727273,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1919191919,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2525252525,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.3080808081,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.303030303,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2676767677,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.3333333333,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2626262626,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.3080808081,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2272727273,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2676767677,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.303030303,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.2222222222,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1565656566,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1515151515,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1414141414,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1363636364,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1414141414,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1666666667,GPQA (Diamond Set),Simple-eval,0.7,1.0,6,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1414141414,GPQA (Diamond Set),Simple-eval,0.7,1.0,7,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1060606061,GPQA (Diamond Set),Simple-eval,0.7,1.0,8,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1262626263,GPQA (Diamond Set),Simple-eval,0.7,1.0,9,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1414141414,GPQA (Diamond Set),Simple-eval,0.7,1.0,10,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1818181818,GPQA (Diamond Set),Simple-eval,0.7,1.0,11,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1313131313,GPQA (Diamond Set),Simple-eval,0.7,1.0,12,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1262626263,GPQA (Diamond Set),Simple-eval,0.7,1.0,13,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1313131313,GPQA (Diamond Set),Simple-eval,0.7,1.0,14,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1616161616,GPQA (Diamond Set),Simple-eval,0.7,1.0,15,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1060606061,GPQA (Diamond Set),Simple-eval,0.7,1.0,16,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1717171717,GPQA (Diamond Set),Simple-eval,0.7,1.0,17,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1767676768,GPQA (Diamond Set),Simple-eval,0.7,1.0,18,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1616161616,GPQA (Diamond Set),Simple-eval,0.7,1.0,19,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral 7B,2023-10-10,,,0.1363636364,GPQA (Diamond Set),Simple-eval,0.7,1.0,20,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Mistral NeMo,2024-07-18,,,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3535353535,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2525252525,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2929292929,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2575757576,GPQA (Diamond Set),Simple-eval,0.7,1.0,6,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2929292929,GPQA (Diamond Set),Simple-eval,0.7,1.0,7,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,1.0,8,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,9,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2626262626,GPQA (Diamond Set),Simple-eval,0.7,1.0,10,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2828282828,GPQA (Diamond Set),Simple-eval,0.7,1.0,11,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,1.0,12,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2474747475,GPQA (Diamond Set),Simple-eval,0.7,1.0,13,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,1.0,14,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,1.0,15,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3535353535,GPQA (Diamond Set),Simple-eval,0.7,1.0,16,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2777777778,GPQA (Diamond Set),Simple-eval,0.7,1.0,17,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2878787879,GPQA (Diamond Set),Simple-eval,0.7,1.0,18,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2929292929,GPQA (Diamond Set),Simple-eval,0.7,1.0,19,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,20,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2878787879,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2424242424,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mistral NeMo,2024-07-18,,,0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo
Mixtral 8x22B,2024-04-17,,,0.3636363636,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.2676767677,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3131313131,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,1.0,6,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.398989899,GPQA (Diamond Set),Simple-eval,0.7,1.0,7,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3333333333,GPQA (Diamond Set),Simple-eval,0.7,1.0,8,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,9,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3888888889,GPQA (Diamond Set),Simple-eval,0.7,1.0,10,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,1.0,11,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3282828283,GPQA (Diamond Set),Simple-eval,0.7,1.0,12,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3333333333,GPQA (Diamond Set),Simple-eval,0.7,1.0,13,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3636363636,GPQA (Diamond Set),Simple-eval,0.7,1.0,14,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,15,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,16,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,1.0,17,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,1.0,18,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,19,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3131313131,GPQA (Diamond Set),Simple-eval,0.7,1.0,20,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3383838384,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3434343434,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3434343434,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x22B,2024-04-17,,,0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Mixtral 8x7B,2023-12-11,,,0.2878787879,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2929292929,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.297979798,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2727272727,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,6,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2626262626,GPQA (Diamond Set),Simple-eval,0.7,1.0,7,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2676767677,GPQA (Diamond Set),Simple-eval,0.7,1.0,8,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.297979798,GPQA (Diamond Set),Simple-eval,0.7,1.0,9,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,10,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,11,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,1.0,12,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3232323232,GPQA (Diamond Set),Simple-eval,0.7,1.0,13,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2727272727,GPQA (Diamond Set),Simple-eval,0.7,1.0,14,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.297979798,GPQA (Diamond Set),Simple-eval,0.7,1.0,15,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2777777778,GPQA (Diamond Set),Simple-eval,0.7,1.0,16,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,1.0,17,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2575757576,GPQA (Diamond Set),Simple-eval,0.7,1.0,18,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,19,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2474747475,GPQA (Diamond Set),Simple-eval,0.7,1.0,20,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2525252525,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3131313131,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2626262626,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.3181818182,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mixtral 8x7B,2023-12-11,,,0.2676767677,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.4393939394,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.404040404,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.4343434343,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.4242424242,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.398989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-V2,2024-05-07,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4343434343,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4444444444,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4444444444,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4191919192,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.404040404,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.4494949495,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
DeepSeek-Coder-V2 236B,2024-06-17,1.2852e+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-coder
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.404040404,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3888888889,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.404040404,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3686868687,GPQA (Diamond Set),Simple-eval,0.95,0.7,1,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3636363636,GPQA (Diamond Set),Simple-eval,0.95,0.7,2,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3737373737,GPQA (Diamond Set),Simple-eval,0.95,0.7,3,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3787878788,GPQA (Diamond Set),Simple-eval,0.95,0.7,4,Zhipu AI,China,API access,BigModel,glm-4
GLM-4 (0520),2024-06-18,1.2e+25,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",0.3131313131,GPQA (Diamond Set),Simple-eval,0.95,0.7,5,Zhipu AI,China,API access,BigModel,glm-4
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2626262626,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.297979798,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2878787879,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2828282828,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3080808081,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.297979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2878787879,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2676767677,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2878787879,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3383838384,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2525252525,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3181818182,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3181818182,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.297979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3131313131,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2676767677,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3434343434,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.297979798,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2575757576,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2676767677,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2878787879,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3434343434,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3131313131,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2929292929,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2676767677,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3232323232,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3686868687,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3232323232,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2878787879,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3232323232,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2727272727,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3232323232,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3484848485,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3080808081,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.297979798,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2777777778,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2626262626,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.2828282828,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.3333333333,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.2878787879,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3131313131,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3181818182,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.3484848485,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4444444444,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4292929293,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.398989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4494949495,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4242424242,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4393939394,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.4141414141,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5101010101,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5050505051,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5101010101,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5101010101,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5202020202,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4949494949,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4949494949,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5202020202,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5353535354,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5555555556,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5050505051,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5353535354,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5202020202,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4444444444,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5252525253,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4747474747,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5050505051,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4494949495,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4646464646,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4848484848,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4797979798,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5202020202,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5050505051,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5404040404,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4797979798,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4949494949,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4747474747,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4848484848,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4848484848,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4545454545,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4898989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5050505051,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5353535354,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4949494949,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5101010101,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5454545455,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4595959596,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4898989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5252525253,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4848484848,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.5050505051,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4696969697,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4242424242,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4898989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
GPT-4o mini (2024-07-18),2024-07-18,,,0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.404040404,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,1.0,6,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4191919192,GPQA (Diamond Set),Llama-eval,0.0,1.0,7,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,1.0,8,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,9,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,10,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,11,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,1.0,12,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3888888889,GPQA (Diamond Set),Llama-eval,0.0,1.0,13,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,1.0,14,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4242424242,GPQA (Diamond Set),Llama-eval,0.0,1.0,15,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,1.0,16,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.398989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,17,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,18,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,19,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,20,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.404040404,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4191919192,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4191919192,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3585858586,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3838383838,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4292929293,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3838383838,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4343434343,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3939393939,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4141414141,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3787878788,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4090909091,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.398989899,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3737373737,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4191919192,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3535353535,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4545454545,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.398989899,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o mini (2024-07-18),2024-07-18,,,0.4191919192,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5202020202,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5555555556,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4848484848,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5505050505,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4646464646,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4747474747,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5050505051,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5757575758,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5050505051,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4898989899,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4949494949,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5202020202,GPQA (Diamond Set),Simple-eval,0.7,1.0,6,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5757575758,GPQA (Diamond Set),Simple-eval,0.7,1.0,7,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5050505051,GPQA (Diamond Set),Simple-eval,0.7,1.0,8,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5555555556,GPQA (Diamond Set),Simple-eval,0.7,1.0,9,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,10,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5505050505,GPQA (Diamond Set),Simple-eval,0.7,1.0,11,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,12,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4848484848,GPQA (Diamond Set),Simple-eval,0.7,1.0,13,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4848484848,GPQA (Diamond Set),Simple-eval,0.7,1.0,14,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5252525253,GPQA (Diamond Set),Simple-eval,0.7,1.0,15,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5252525253,GPQA (Diamond Set),Simple-eval,0.7,1.0,16,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5202020202,GPQA (Diamond Set),Simple-eval,0.7,1.0,17,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.5101010101,GPQA (Diamond Set),Simple-eval,0.7,1.0,18,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4595959596,GPQA (Diamond Set),Simple-eval,0.7,1.0,19,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4747474747,GPQA (Diamond Set),Simple-eval,0.7,1.0,20,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-405B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4595959596,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.5,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4696969697,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4191919192,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4545454545,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4494949495,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4797979798,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.4696969697,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-70B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2171717172,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2777777778,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2373737374,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2121212121,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.3131313131,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.303030303,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Llama 3.1-8B,2024-07-23,,,0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Meta AI,United States of America,Open weights (restricted use),Hyperbolic,Meta-Llama-3.1-8B-Instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4444444444,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4595959596,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4292929293,GPQA (Diamond Set),Simple-eval,0.2,0.7,1,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4141414141,GPQA (Diamond Set),Simple-eval,0.2,0.7,2,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4595959596,GPQA (Diamond Set),Simple-eval,0.2,0.7,3,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.4090909091,GPQA (Diamond Set),Simple-eval,0.2,0.7,4,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
Nemotron-4 340B,2024-06-14,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",0.398989899,GPQA (Diamond Set),Simple-eval,0.2,0.7,5,NVIDIA,United States of America,Open weights (unrestricted),NVIDIA,nemotron-4-340b-instruct
o1-mini,2024-09-12,,,0.6212121212,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5656565657,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6161616162,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6262626263,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6414141414,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6060606061,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5707070707,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5808080808,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6363636364,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5808080808,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6313131313,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6313131313,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6313131313,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6111111111,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6161616162,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6161616162,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6111111111,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.6666666667,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.595959596,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.595959596,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5808080808,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5808080808,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.5858585859,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-mini,2024-09-12,,,0.601010101,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-preview,2024-09-12,,,0.7070707071,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6717171717,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6868686869,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6818181818,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.702020202,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6868686869,GPQA (Diamond Set),Simple-eval,1.0,1.0,6,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7070707071,GPQA (Diamond Set),Simple-eval,1.0,1.0,7,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.702020202,GPQA (Diamond Set),Simple-eval,1.0,1.0,8,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6868686869,GPQA (Diamond Set),Simple-eval,1.0,1.0,9,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6919191919,GPQA (Diamond Set),Simple-eval,1.0,1.0,10,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6515151515,GPQA (Diamond Set),Simple-eval,1.0,1.0,11,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,12,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7373737374,GPQA (Diamond Set),Simple-eval,1.0,1.0,13,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6616161616,GPQA (Diamond Set),Simple-eval,1.0,1.0,14,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6818181818,GPQA (Diamond Set),Simple-eval,1.0,1.0,15,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,16,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6767676768,GPQA (Diamond Set),Simple-eval,1.0,1.0,17,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7676767677,GPQA (Diamond Set),Simple-eval,1.0,1.0,18,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7323232323,GPQA (Diamond Set),Simple-eval,1.0,1.0,19,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6666666667,GPQA (Diamond Set),Simple-eval,1.0,1.0,20,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7121212121,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.6919191919,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.696969697,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7121212121,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
o1-preview,2024-09-12,,,0.7222222222,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2828282828,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2626262626,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2929292929,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.3585858586,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2777777778,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.2676767677,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
DBRX,2024-03-27,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.3232323232,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Databricks,United States of America,Open weights (restricted use),Together,dbrx-instruct
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3434343434,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3787878788,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3484848485,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3636363636,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.4191919192,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3131313131,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3131313131,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3282828283,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3484848485,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3181818182,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.2878787879,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3181818182,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.3181818182,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.4595959596,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.4494949495,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.3888888889,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.3939393939,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.3535353535,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.3888888889,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.3484848485,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.4393939394,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.3636363636,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.2828282828,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.2727272727,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.2626262626,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.303030303,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.3080808081,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.297979798,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Llama 3-8B,2024-04-18,7.2e+23,,0.2878787879,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.2474747475,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.3282828283,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.3232323232,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.303030303,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.2828282828,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.297979798,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5-110B,2024-04-25,,lower bound is taken from Qwen1.5 72B training compute estimation,0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Alibaba,China,Open weights (unrestricted),Together,Qwen1.5-110B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3585858586,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.2727272727,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.2525252525,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3131313131,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.2676767677,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3232323232,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.3282828283,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3434343434,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3787878788,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4191919192,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3535353535,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.398989899,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.404040404,GPQA (Diamond Set),Llama-eval,0.0,0.7,6,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3636363636,GPQA (Diamond Set),Llama-eval,0.0,0.7,7,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,0.7,8,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,0.7,9,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4343434343,GPQA (Diamond Set),Llama-eval,0.0,0.7,10,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3737373737,GPQA (Diamond Set),Llama-eval,0.0,0.7,11,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4242424242,GPQA (Diamond Set),Llama-eval,0.0,0.7,12,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,0.7,13,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,0.7,14,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,0.7,15,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4292929293,GPQA (Diamond Set),Llama-eval,0.0,0.7,16,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,0.7,17,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,0.7,18,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3939393939,GPQA (Diamond Set),Llama-eval,0.0,0.7,19,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.3585858586,GPQA (Diamond Set),Llama-eval,0.0,0.7,20,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.1363636364,GPQA (Diamond Set),Simple-eval,0.7,0.7,1,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.1767676768,GPQA (Diamond Set),Simple-eval,0.7,0.7,2,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.1565656566,GPQA (Diamond Set),Simple-eval,0.7,0.7,3,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.1616161616,GPQA (Diamond Set),Simple-eval,0.7,0.7,4,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.1919191919,GPQA (Diamond Set),Simple-eval,0.7,0.7,5,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.297979798,GPQA (Diamond Set),Llama-eval,0.0,0.7,1,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.2929292929,GPQA (Diamond Set),Llama-eval,0.0,0.7,2,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.2777777778,GPQA (Diamond Set),Llama-eval,0.0,0.7,3,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.2525252525,GPQA (Diamond Set),Llama-eval,0.0,0.7,4,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Yi-34B,2023-11-02,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.2373737374,GPQA (Diamond Set),Llama-eval,0.0,0.7,5,01.AI,China,Open weights (restricted use),Together,Yi-34B-Chat
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.3636363636,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.3585858586,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.3686868687,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2727272727,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2424242424,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2575757576,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2575757576,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2272727273,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.04545454545,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.07575757576,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.04545454545,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.07070707071,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.06060606061,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.4191919192,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Hermes 2 Theta Llama-3 70B,2024-06-20,6.3e+24,,0.4191919192,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Nous Research,United States of America,Open weights (unrestricted),vLLM,Hermes-2-Theta-Llama-3-70B
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2474747475,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2424242424,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2525252525,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2525252525,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2575757576,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Qwen1.5. 32B,2024-04-25,,,0.2626262626,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.2373737374,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.2373737374,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.2424242424,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.2323232323,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.202020202,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.2272727273,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.2171717172,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.2171717172,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Yi-1.5-34B,2024-05-13,7.344e+23,6*34*10^9*3.6*10^12 = 7.344e+23,0.1818181818,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,01.AI,China,Open weights (restricted use),vLLM,Yi-1.5-34B-Chat
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2070707071,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2070707071,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2070707071,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2070707071,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
Llama 2-70B,2023-07-18,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.2070707071,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Meta AI,United States of America,Open weights (restricted use),vLLM,Llama-2-70b-chat-hf
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.202020202,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.1767676768,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2171717172,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2323232323,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
DeepSeek LLM 67B,2024-01-05,8.04e+23,67B * 2T * 6 = 8.04e23,0.2272727273,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,DeepSeek,China,Open weights (restricted use),vLLM,deepseek-llm-67b-chat
Qwen1.5. 32B,2024-04-25,,,0.1717171717,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.202020202,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.1818181818,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.1868686869,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Qwen1.5. 32B,2024-04-25,,,0.1818181818,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Qwen,China,Open weights (unrestricted),vLLM,Qwen1.5-32B-Chat
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.6060606061,GPQA (Diamond Set),Simple-eval,1.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.6060606061,GPQA (Diamond Set),Simple-eval,1.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5757575758,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5757575758,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5707070707,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5656565657,GPQA (Diamond Set),Simple-eval,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5606060606,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5555555556,GPQA (Diamond Set),Simple-eval,1.0,1.0,5,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5505050505,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4393939394,GPQA (Diamond Set),Simple-eval,1.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4242424242,GPQA (Diamond Set),Simple-eval,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4191919192,GPQA (Diamond Set),Simple-eval,1.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4191919192,GPQA (Diamond Set),Simple-eval,1.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4141414141,GPQA (Diamond Set),Llama-eval,0.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4090909091,GPQA (Diamond Set),Llama-eval,0.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.404040404,GPQA (Diamond Set),Llama-eval,0.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.404040404,GPQA (Diamond Set),Llama-eval,0.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.3838383838,GPQA (Diamond Set),Llama-eval,0.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.3686868687,GPQA (Diamond Set),Simple-eval,1.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.5353535354,GPQA (Diamond Set),Simple-eval,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.5,GPQA (Diamond Set),Simple-eval,1.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4949494949,GPQA (Diamond Set),Simple-eval,1.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4696969697,GPQA (Diamond Set),Llama-eval,0.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4646464646,GPQA (Diamond Set),Llama-eval,0.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4646464646,GPQA (Diamond Set),Simple-eval,1.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4646464646,GPQA (Diamond Set),Simple-eval,1.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4545454545,GPQA (Diamond Set),Llama-eval,0.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4494949495,GPQA (Diamond Set),Llama-eval,0.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4343434343,GPQA (Diamond Set),Llama-eval,0.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.5404040404,GPQA (Diamond Set),Simple-eval,1.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.5151515152,GPQA (Diamond Set),Simple-eval,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.5,GPQA (Diamond Set),Simple-eval,1.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4797979798,GPQA (Diamond Set),Simple-eval,1.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4747474747,GPQA (Diamond Set),Simple-eval,1.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4646464646,GPQA (Diamond Set),Llama-eval,0.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4595959596,GPQA (Diamond Set),Llama-eval,0.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4545454545,GPQA (Diamond Set),Llama-eval,0.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Flash (2024-08-27),2024-08-27,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.4494949495,GPQA (Diamond Set),Llama-eval,0.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-exp-0827
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4797979798,GPQA (Diamond Set),Llama-eval,0.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4747474747,GPQA (Diamond Set),Llama-eval,0.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4747474747,GPQA (Diamond Set),Simple-eval,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4646464646,GPQA (Diamond Set),Simple-eval,1.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4595959596,GPQA (Diamond Set),Simple-eval,1.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4545454545,GPQA (Diamond Set),Llama-eval,0.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4444444444,GPQA (Diamond Set),Simple-eval,1.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4393939394,GPQA (Diamond Set),Llama-eval,0.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.4292929293,GPQA (Diamond Set),Simple-eval,1.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.6363636364,GPQA (Diamond Set),Simple-eval,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.595959596,GPQA (Diamond Set),Llama-eval,0.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5757575758,GPQA (Diamond Set),Simple-eval,1.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5757575758,GPQA (Diamond Set),Llama-eval,0.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5707070707,GPQA (Diamond Set),Llama-eval,0.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5606060606,GPQA (Diamond Set),Llama-eval,0.0,0.95,4,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5606060606,GPQA (Diamond Set),Simple-eval,1.0,0.95,2,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5555555556,GPQA (Diamond Set),Simple-eval,1.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5555555556,GPQA (Diamond Set),Simple-eval,1.0,0.95,5,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.5505050505,GPQA (Diamond Set),Llama-eval,0.0,0.95,3,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3787878788,GPQA (Diamond Set),Llama-eval,0.0,1.0,5,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3686868687,GPQA (Diamond Set),Llama-eval,0.0,1.0,2,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3636363636,GPQA (Diamond Set),Simple-eval,0.7,1.0,3,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3535353535,GPQA (Diamond Set),Llama-eval,0.0,1.0,1,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3484848485,GPQA (Diamond Set),Llama-eval,0.0,1.0,4,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3484848485,GPQA (Diamond Set),Simple-eval,0.7,1.0,2,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3434343434,GPQA (Diamond Set),Simple-eval,0.7,1.0,1,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3383838384,GPQA (Diamond Set),Simple-eval,0.7,1.0,4,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3333333333,GPQA (Diamond Set),Llama-eval,0.0,1.0,3,Mistral AI,France,API access,Mistral,mistral-large-2402
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.3080808081,GPQA (Diamond Set),Simple-eval,0.7,1.0,5,Mistral AI,France,API access,Mistral,mistral-large-2402
o1-mini,2024-09-12,,,0.8111782477,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,o1-mini-2024-09-12
o1-preview,2024-09-12,,,0.6805135952,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,o1-preview-2024-09-12
Gemini 1.5 Pro (2024-09-24),2024-09-24,,,0.6729607251,MATH 5,,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-002
Qwen2.5-72B,2024-09-19,7.8e+24,"Training dataset size was 18 trillion

6 * 72.7 billion * 18 trillion = 7.8e24",0.583081571,MATH 5,,0.7,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen2.5-72B-Instruct-Turbo
Gemini 1.5 Flash (2024-09-24),2024-09-24,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.583081571,MATH 5,,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-002
Claude 3.5 Sonnet (2024-10-22),2024-10-22,,,0.5279456193,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20241022
GPT-4o mini (2024-07-18),2024-07-18,,,0.4803625378,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-mini-2024-07-18
GPT-4o (2024-05-13),2024-05-13,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4780966767,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-05-13
GPT-4o (2024-08-06),2024-08-06,,"Not known. But it's more capable than GPT-4, Gemini 1 Ultra, etc",0.4728096677,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4o-2024-08-06
Claude 3.5 Sonnet (2024-06-20),2024-06-20,,,0.4607250755,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-5-sonnet-20240620
DeepSeek-V2.5,2024-09-06,1.7892e+24,"V2.5 is a merge of V2-coder and V2-chat
V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T
V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training
Unique steps: 8.2T + 6T = 14.2T
FLOPs: 6 * 21B * 14.2T = 1.7892e24",0.4561933535,MATH 5,,1.0,1.0,1,DeepSeek,China,Open weights (restricted use),DeepSeek,deepseek-chat
Llama 3.1-405B,2024-07-23,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.4478851964,MATH 5,,1.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),SambaNova,Meta-Llama-3.1-405B-Instruct
Mistral Large 2,2024-07-24,,,0.4373111782,MATH 5,,0.7,1.0,1,Mistral AI,France,Open weights (non-commercial),Mistral,mistral-large-2407
Grok-2 Beta,2024-08-13,,,0.4040785498,MATH 5,,1.0,1.0,1,xAI,United States of America,Hosted access (no API),XAI,grok-beta
Gemini 1.5 Pro (2024-02-15),2024-02-15,,,0.3617824773,MATH 5,,0.9,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-pro-001
GPT-4 Turbo (2023-11-06),2023-11-06,,,0.3610271903,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-1106-preview
Claude 3 Opus,2024-03-04,,,0.3406344411,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-opus-20240229
GPT-4 Turbo (2024-01-25),2024-01-25,,,0.336102719,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-0125-preview
Gemma 2 27B,2024-06-24,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.2348942598,MATH 5,,0.7,0.7,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-27b-it
Mixtral 8x22B,2024-04-17,,,0.2326283988,MATH 5,,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x22b
Gemini 1.5 Flash (2024-05-10),2024-05-10,,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""
So Flash implicitly includes training compute of 1.5 Pro.",0.2296072508,MATH 5,,1.0,0.95,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",API access,Gemini,gemini-1.5-flash-001
Mistral Large,2024-02-26,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.2137462236,MATH 5,,0.7,1.0,1,Mistral AI,France,API access,Mistral,mistral-large-2402
GPT-4 (2023-06-13),2023-06-13,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.2129909366,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-4-0613
Claude 3 Sonnet,2024-03-04,,,0.1616314199,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-sonnet-20240229
GPT-3.5 Turbo (2023-11-06),2023-11-06,,,0.1495468278,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-1106
Claude 3 Haiku,2024-03-04,,,0.1329305136,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-3-haiku-20240307
Ministral 3B,2024-02-26,,,0.1185800604,MATH 5,,0.7,1.0,1,Mistral AI,France,API access,Mistral,ministral-3b-2410
Claude 2.1,2023-11-21,,,0.1148036254,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-2.1
Ministral 8B,2024-02-26,,,0.1125377644,MATH 5,,0.7,1.0,1,Mistral AI,France,API access,Mistral,ministral-8b-2410
GPT-3.5 Turbo (2024-01-25),2024-01-25,,,0.1080060423,MATH 5,,1.0,1.0,1,OpenAI,United States of America,API access,OpenAI,gpt-3.5-turbo-0125
Claude 2,2023-07-11,,,0.1019637462,MATH 5,,1.0,1.0,1,Anthropic,United States of America,API access,Anthropic,claude-2.0
Mistral NeMo,2024-07-18,,,0.09818731118,MATH 5,,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-nemo-2407
Mixtral 8x7B,2023-12-11,,,0.07401812689,MATH 5,,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mixtral-8x7b
Mistral 7B,2023-10-10,,,0.02945619335,MATH 5,,0.7,1.0,1,Mistral AI,France,Open weights (unrestricted),Mistral,open-mistral-7b
Qwen2-72B,2024-06-07,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.4342900302,MATH 5,,0.7,0.7,1,Alibaba,China,Open weights (unrestricted),Together,Qwen2-72B-Instruct
Llama 3.1-70B,2024-07-23,7.929e+24,,0.3889728097,MATH 5,,1.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),SambaNova,Meta-Llama-3.1-70B-Instruct
Llama 3-70B,2024-04-18,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.2235649547,MATH 5,,0.7,0.7,1,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-70b-chat-hf
Wizard LM 2 8x22B,2024-02-26,,,0.2228096677,MATH 5,,0.7,0.7,1,Microsoft,United States of America,Open weights (restricted use),Together,WizardLM-2-8x22B
Llama 3.1-8B,2024-07-23,,,0.2160120846,MATH 5,,1.0,1.0,1,Meta AI,United States of America,Open weights (restricted use),SambaNova,Meta-Llama-3.1-8B-Instruct
Qwen1.5 72B,2024-02-04,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.1941087613,MATH 5,,0.7,0.7,1,Alibaba,China,Open weights (restricted use),Together,Qwen1.5-72B-Chat
Gemma 2 9B,2024-06-24,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6*9000000000*8000000000000=4.32e+23",0.1835347432,MATH 5,,0.7,0.7,1,Google DeepMind,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),Together,gemma-2-9b-it
Llama 3-8B,2024-04-18,7.2e+23,,0.07930513595,MATH 5,,0.7,0.7,1,Meta AI,United States of America,Open weights (restricted use),Together,Llama-3-8b-chat-hf
